# 1. Introduction
*Organisatorisches ausgelassen. Slides sind English,*
*meine Notizen also auch.*


## System programming
- Either or both:
  - Focus on system software (services provided to applications)
  - Foucs on performance/resource constraints (high-perf computing)
- Used langs usually with lightweight runtime, garbage collection rare

### C & C++
- Course will use these 2
- C closely assotiated with UNIX
  - General-purpose; manual memory management, often used for 
    systems code
- C++ bloted superset of C with a bias towards systems programming
  - Claims to combine procedural, OO, and generic programming


## 3 Kinds of parallelism
- Thread-level: Software-managed parallelism, running on multiple cores/CPUs
- Instruction-level: Pipelining, out-of-order execution, SIMD
- Low-level digital design: CPU caches, carry-lookahead, prefix adders


##  Parallel programming
Defined as making use of thread-level parallelism.
Main use lies in compute-intensive applications, many of which
can be performed in parallel

Reason this is useful: Single cores hit a performance wall,
more speed is not possible. Moore's law of exponential
growth does not apply anymore.

### Issues with faster unicores
- With smaller transistors, the leakage current stays constant, and
  so energy use becomes the barrier as transistors get smaller. 
  Most modern CPUs limited to 4Ghz clock before energy consumption
  becomes too high to keep up with cooling
- Memory performance increase has been substantially slower,
  memory is a bottleneck
- Instruction level parallelism such as pipelining and branch
  prediction is reaching diminishing returns,
  any more needs too much die size to be worth implementing

### Multicores
- Multiple cores in 1 die/chip
- Cores can be heterogeneous; specialized cores for different applications
- Extreme variant: GPUs, often thousands of cores. GPU computing
  possible by uploading workload to GPU and getting results
  back into main memory

### Why don't compilers do this?
- Making sequential programs parallel automatically is very difficult
- Rarely applicable as most opportunities are not visible to compiler

### Difference to concurrency
Parallelism: Parallel execution of tasks, running at the same time  
Concurrency: Overlapping tasks, switching between is concurrent but
not parallel

### Types of parallelism
- Functional parallelism: Problem as a set of functions
  - Execute those simultaneously
  - Also called task parallelism
  - Limited scalability
- Data parallelism: Problem as a set of data
  - Break it down into chunks that can be processed individually
  - Process them concurrently
  - Scales with data, suitable for multicores


## Writing Parallel Programs
- Often, sequential programs are made parallel instead of from-scratch parallelism
- Benefits: Speed, sometimes cleaner, lower energy consumption
- Cost: Effort, complexity, possible overhead, bugs, dependencies, non-determinism

### Obstacles
- Control dependencies: Work is only performed conditionally,
  checking condition prevents parallelization
- Data dependencies: Data depends on other data to be calculated
  first, prevents parallelization

