# 4. Parallel Architectures
### Flynn's classification
Same as the one presented in RO:

- SISD: Classic uni-CPU
- SIMD: Data parallelism
- MIMD: Full CPU parallelism

### Single-instruction multiple threads
SIMT, used by GPUs. Threads are grouped into *warps*,
every time an instruction is issued the instruction unit
selects the warp that is ready for the next instruction and
broadcasts it.

### MIMD
- Popular for high performance applications
- Cheap, can use regular CPUs, flexible
- Often SPMD - single program, multiple data


## Memory
### Uniform Memory Access (UMA)
- Same access time for all CPUs for all memory
- Simple, doesn't scale well; usually means that all CPUs need to
  share a bus to memory

### Non-UMA (NUMA)
- Memory has affinity to processor
- Local memory is faster than remote
- More complex but scalable

### Distributed
- Some CPUs share a section of the memory
- Other CPUs need to access that memory over some network shared
  between all CPUs
- Used in multicomputer clusters


## Network
- Physical link between components, usually CPUs/memory or nodes
- Communication via messaging
- *Topology*: Layout of links/switches
- *Routing technique*: Paths of messages through link
- *Bandwidth*: Rate of information transfer
  - *aggregate*: Total bandwidth in the network
  - *effective*/*Throughput*: Bandwidth available to an application
- *Latency*: Overhead of sending/receiving a message

### Shared-media networks
- Only one message at a time
- Shared, single bus
- Each processor can listen to messages, only reacts to ones it is 
  the destination for
- Low cost, does not scale

### Switched-media networks
- Supports P2P between nodes
- Own communication path for all nodes
- Supports concurrent messages, scalable
- Often centralized networks, where each CPU is connected indirectly
  through several links and intermediate switches
- Distinction between centralized and distributed SMN

### Topologies
- *Crossbar switch*: Non-blocking, links not shared
  - Requires $n^2$ switches, scalability limited
- *Multistage interconnection network*:
  - Blocking due to shared network links
  -  Perfect shuffle permutation
- *Fat tree*: Balanced tree where leaves are nodes, vertices switches
  - Total bandwidth constant
  - Switches often composed of multiple smaller switches
  - Popular for cluster interconnects
- *Distributed switched networks*: Each switch has end devices
  attached, ratio of 1:1 for nodes/switches
- *Torus*: Mesh with wraparound connections, each node has 2 neighbors
  per dimension, usually 3-6 dimensions
- *Dragonfly*: Arbitrary networks for intra- and inter-group networks

Criteria for evaluation:

- *Network degree*: $max(n)$, $n_node$ = number of adjacent nodes
- *Diameter*: Largest minimum distance between 2 nodes
- *Bisection bandwidth*: Partition network in 2, so that
  aggregate bandwidth between halves is minimal
- *Edge connectivity*: Minimum edges/nodes that need to be removed
  to bring down network

Requirements:

- Low network degree (= lower cost)
- Low diameter (= lower latency)
- High bisection bandwidth (= high throughput)
- High connectivity (= more robust)


## Lichtenberg Cluster
- Cluster owned/managed by the TU
- Energy consumption: 620kW at full load
- Linux compute nodes with infiniband interconnect (100GBit/s)
- Unified management, non-interactive task computation, job
  scheduler to allow running jobs
- Hot-water cooling: components directly cooled by cooling fluid at
  around 40-55C


## Cache
- Cache coherence important
  - Read after write by same CPU always returns new value
  - Read after write by different CPU returns new value if enough
    time since write
  - Writes are serial

### Coherence protocols
- Snooping: No centralized state
  - Cache keeps track of sharing status
  - Caches snoop on memory to see if they have a copy of a block
    being requested by some other cache
- Directory-based: Sharing status kept centralized
  - Must be distributed when scaling big

### Semantics
- Shared memory intuitive, but quite complex (cache hierarchy,
  invalidation, sharing, etc.)
- Order of updates can be influenced by:
  - Order in which memory requests arrive
  - Instruction reordering by compiler

### Consistency
- Ordering of accesses to different memory
- *Sequential*: All access is sequential, in order defined by the
  program (simple but slow)
- Relaxed models: Allow out-of-order R/W, only enforce sequential
  R/W where necessary for synchronization
  - Sync still necessary to prevent e.g. data races
  - Can be a bottleneck
