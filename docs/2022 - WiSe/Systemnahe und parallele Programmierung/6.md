# 6. Parallel Programming Models
Abstraction for parallel computing, goal of increasing productivity
and portability.  
SIMD is the underlying idea of most models.

### Message passing
Threads communicate using a shared link and a message buffer;
example MPI with functions `MPI_Send`, `MPI_Recv`.  
Scalable, but requires redesign for existing code.

### Multithreading
Threads communicate using shared memory, synchronization through
barriers, locks and atomic operations. Example: OpenMP.  
Incrementally applicable, but limited scalability and hard to debug.

### GPGPU computing
Often used for data-parallel workloads; workload gets uploaded
the GPU, which can run highly parallel. Example: OpenCL.  
Efficient and scalable, but complex and hard to optimize.
