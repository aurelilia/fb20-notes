# 10. Parallel Algorithms
## Models
Algorithms can be represented using multiple different models allowing
reasoning about their behavior and efficiency.

### DAG (Directed acyclic graph)
- Inputs are leaf nodes
- Outputs are root nodes (with no parents)
- Inner nodes are compute operations, with their children being inputs
- Depth of DAG directly correlates to amount of data dependencies
- Arvind built Monsoon: data flow computer with parallel languages, in 2008
  
### Shared memory
- Natural extension of sequential algorithm notation
- Local and shared memory for each processing unit
- A/synchronous models with shared/separate clocks per unit possible
- PRAM (synchronous shared memory) variants:
    - EREW: No simultaneous access to shared memory
    - CREW: Simultaneous access when reading only
    - CRCW: Simultaneous access R/W

### Network
- System as graph $G = (N, E)$
- Processors as $i \in N$
- Edge $(i, j) \in E$ bidirectional communication link
- No shared memory, message-passing model
- Network topology important; see previous lectures on supercomputers

### Work-depth model
- Program represented as a DAG
- Greedy scheduling (work queue)
- Useful for reasoning about performance on task-based algorithms
- Still simplification!

### LogP model
- L: Latency of medium
- o: Overhead of send/receive
- g: Required gap between send/receive operations
- P: Number of processing units

### Comparison
- DAG: Hard to reason about at scale; scheduling not considered
- PRAM: Assumption that communication is free, no bandwidth limits
- Network: Difficult to describe, topology restricts algorithms


## Performance
Performance is usually measured in the worst-case.
