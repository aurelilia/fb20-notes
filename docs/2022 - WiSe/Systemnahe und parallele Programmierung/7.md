# 7. OpenMP
Open Multi-Processing, community standard for shared-memory parallelism
for Fortran/C(++). Predecessors were various proprietary designs by vendors.


## API components
### Directives
Comments in Fortran, Pragmas in C/C++; need compiler support.
Allow for express parallelism including synchronization.  
They are ignored by compilers without OMP support.

### Library functions
Allow interacting with the MP runtime, along with synchronization
primitives.

### Environment variables
... Can be used to specify execution parameters to the runtime.

### Supported execution models
- Fork-join: Some threads get spawned and implicit tasks produced from 
  data set get distributed; after all are done, program joins them
  with a barrier and is sequential again.
- Work Sharing: Explicitly split pieces of work get distributed across
  multiple threads
- Tasking: A task queue is used; threads take tasks from the queue
  and run them when done with previous task
- Models not covered in SPP:
    - Nested parallelism: Threads fork again
    - Offloading computations: Offloading to different devices
    - SIMD


## Thread Model
- Each thread has own data environment, consisting of:
  - Globals
  - Automatic variables allocated on-stack
  - Dynamically allocated variables on-heap
- Initial thread's data environment has program lifetime
- Worker threads have own stack

### Sharing semantics
- Shared semantics:
    - Variable always points to same data location
    - Communication between workers with variables
- Private semantics:
    - One store per thread
    - Inaccessible to other threads
- Reduction variables
    - In between either (?)

### Memory model
- Load/store is not guaranteed to be atomic
- Thread-local storage may not be modified anywhere else

### Internal control variables
- Control the OMP runtime
- Configured through environment variables or usage of the library
- Can be retrieved using library
- Controls number of threads, scheduling strategy, etc.
- Example:
  - `OMP_NUM_THREADS=4`
  - `omp_set_nun_threads(4)`


## Example
```c
#include <omp.h>
#include <stdio.h>

int main(int argc, char* argv[]) {
    // SPMD, this will print 0-thread_num
    #pragma omp parallel
    printf("%d\n", omp_get_thread_num());

    // Loop-level
    #pragma omp parallel for
    for (int i = 0; i < n; i++) {
        a[i][0] = 0;
    }
    }
```


## Loop-level parallelism
Execution of loop iterations concurrently, using a directive followed
by a loop. Iterations get split up evenly among worker threads.
All variables from outside the directive are shared by default;
private inside the directive.

Parallel overhead must be considered; some loops are faster sequentially
due to overhead from threading and work distribution.

### Canonical structure
- Restriction on loop to allow easier parallelization
- Program must complete all iterations (no `break`, no exceptions)
- `continue` and program exit still allowed
- Nesting of loop-level parallel directives forbidden

### Directive clauses
... Can be appended to the directive

| Clause                    | Purpose                                          |
| ------------------------- | ------------------------------------------------ |
| `if(expr)`                | Conditional parallelization                      |
| `num_threads(int)`        | Number of threads                                |
| `copyin(list)`            | Copying between thread-private variables         |
| `proc_bind(...)`          | Thread affinity                                  |
| `collapse(n)`             | Collapses nested loop into one iteration space   |
| `ordered[(n)]`            | Execution order same as in serial execution      |
| `allocate([alloc:]list)`  | Specifies memory allocator for private variables |
| `order([mod:]concurrent)` | Execution order for associated loops             |

### Data dependencies
- Can be anywhere and lead to race conditions
- "Loop-carried dependencies" (i.e. using values from prev. iteration)
- OMP tries to auto-detect loop dependencies
- Flow dependence: read-after-write
- Anti-dependence: write-after-read
- Output dependence: write-after-write

### Advantages
- Allows incremental parallelization of existing code
- Small increase in code size
- Code stays readable and can still run on single-CPU system
- Single address space

### Disadvantages
- Scaling is limited
- Requires compiler support
- Communication is implicit and therefore hidden costs
- Synchronization can sometimes be incorrect, leading to data races, etc.


## Scheduling
Scheduling determines execution distribution and order; correct programs
must not depend on scheduling.

### Static
- Work is distributed to threads upfront
- Therefore deterministic, low overhead
- Less flexible to the workload

### Dynamic
- Distribution is done live during execution
- Synchronization overhead is larger
- Can adjust to load imbalance much easier, as threads request more
  work on-the-fly

### Strategies
- Iterations in loop-level parallelism get distributed into chunks
- Those chunks are then scheduled statically or dynamically.


## SPMD: Work sharing
- Threads execute same code with different data; control flow and similar
  means that threads can still differ significantly
- SPMD global; loop-level local
- SPMD code section called parallel region
  - Must be structured block
  - No branches in/outside the region
  - Branches/termination within region permitted
- Thread identified with `omp_get_thread_num()`

### threadprivate
Directive that makes a variable thread-local across entire program,
initialized once on first use in a thread. Persists across regions.

```c
int my_start, my_end;
#pragma omp threadprivate(my_start, my_end)
```

### copyin
Copies a value to a threadprivate of all threads simultaneously. 

```c
int c;
#pragma omp threadprivate(c)

int main(int argc, char* argv[]) {
    c = 2;
    #pragma omp parallel copyin(c)
    {/* c has value 2 in all thread-private copies */}
}
```

### Work assignment
- Domain decomposition
    - Based on thread count and number
    - Simply dividing work by thread id
- Work sharing
    - Division of loop iterations
    - Distribution of code sections
    - Identification of code that is needed to be executed by a single thread,
      ex. IO. Use `#pragma omp single ...`
        - OMP picks arbitrary thread for execution
- Task construct
    - Define units of work to be assigned to threads

### Comparison to loop-level
Loop-level:

- Only work-sharing
- Iterations must be independent
- Special case of parallel region
- Easy to read/use
- Suited for incremental parallelization
- Limited

SPMD:

- Combination of replicated execution with work-sharing
- More complex to use
- More general and applicable to larger regions
- Less parallelization overhead


## Synchronization
### Mutual exclusion
- Example: critical sections, mutexes
- Exclusive access to shared structure
- OMP: Critical, Atomic, Locks
- Critical: `#pragma omp critical`
    - Only one thread may execute a critical section at one time
- Atomic: `#pragma omp atomic`
- Locks: `omp_lock_t`, `omp_(un)set_lock(&lock)`, `omp_destroy_lock(&lock)`

### Event synchronization
- Example: barrier
- Signal events between threads
- Can be used to implement ordering
- Does not control order of data access
- OMP: barriers, Ordered, task groups, Flush
- Barriers: `#pragma omp barrier`

### Race conditions
- Unenforced relative timing, causing dependence on execution order
- Results in non-determinism
- Failure when deterministic behavior is expected
- Example: Floating-point addition is not associative 

### Data race
- Non-atomic concurrent access resulting in undefined behavior
- Precisely defined by C(++)11: "The execution of a program contains a data
  race if it contains two conflicting actions in different threads, at
  least one of which is not atomic, and neither happens before the other."


## Tasking
Typical task executor and scheduler that distributes tasks across workers.  
Tasks are written as:

```c
#pragma omp task
{ 
    // task code
    // Taskwait can be used to wait on child tasks started inside
    // this task:
    #pragma omp taskwait
    // It does not wait on children of children.

    // Threads may suspend execution when waiting in a task; or using
    #pragma omp taskyield
    // To explicitly yield the current task. 
}
```

This automatically schedules the task; thread or execution time is undefined,
same as task order.  
Tasks are used to implement loop-level and SPMD parallelism internally.

### Dependencies
Task dependencies can be specified to ensure that the scheduler runs
the tasks in order, preventing race conditions and ensuring that the
program does not stall waiting on tasks.

```c
int a, b;
#pragma omp task depend(out:a,b) shared(a,b) // Task A
{}
#pragma omp task depend(inout:a) shared(a) // Task B
{}
#pragma omp task depend(in:b) shared(b) // Task C
{}
#pragma omp task depend(in:a,b) shared(a,b) // Task D
{}
```

Would cause execution order: A -> B & C -> D.
